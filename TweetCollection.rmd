---
title: "TweetCollection"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, include=FALSE}
library(tidyverse)
library(rtweet)
library(purrr)
library(plyr)
library(text2vec)
library(caret)
library(randomForest)
library(caTools)
library(diveRsity)
library(tm)
library(tokenizers)
library(glmnet)
```

POC
```{r}
trump <- get_timeline("realDonaldTrump", n = 180)
```
the tweet collection gives us 90 different variables. Surely we dont need all of that so we are gonna cherry pick the most interesting ones
```{r}
toKeep <- c("screen_name", "text", "is_retweet")
trump <- trump[names(trump) %in% toKeep]
```
Next  I will choose the tweeters for this project. I'm aiming to choose people who are currently in parlament and equally people from each party.

* Outi Alanko-Kahiluoto @outialanko (Green party)
* Pekka Haavisto @Haavisto (Green Party)
* Paavo Arhinmäki @paavoarhinmaki (Left Alliance)
* Mai Kivelä @MaiKivela (Left Alliance)
* Jussi Halla-Aho @Halla_aho (Finns Party)
* Sebastian Tynkkynen @SebastianTyne (Finns Party)
* Terhi Koulumies @terhikoulumies (National Coalition Party)
* Jaana Pelkonen @JaanaPelkonen (National Coalition Party)

```{r}
Tweeters <- c("outialanko", "Haavisto", "paavoarhinmaki", "MaiKivela", "Halla_aho", "Sebastiantyne", "terhikoulumies", "JaanaPelkonen")
TweetersDF <- tibble(name = Tweeters)

GreenParty <- c("OutiAlanko", "PekkaHaavisto")
LeftAlliance <- c("PaavoArhinmaki", "MaiKivela")
FinnsParty <- c("JussiHalla-Aho", "SebastianTynkkynen")
NCP <- c("TerhiKoulumies", "JaanaPelkonen")
```

```{r}
tweetsAll <- TweetersDF %>% mutate(data = map(name, ~get_timeline(.x, n = 1000)))
list2env(tweetsAll, .GlobalEnv) # Separating the lists
AllTweetsDF <- bind_rows(data)  # Unlisting and binding the nested dataframes
```
Cleaning the data
```{r}
toKeep <- c("screen_name", "text", "is_retweet")
AllTweetsDF <- AllTweetsDF[,names(AllTweetsDF) %in% toKeep] # Choose only desired variables

AllTweetsDF <- AllTweetsDF[(AllTweetsDF$is_retweet == "FALSE"),]

drops <- c("is_retweet")
AllTweetsDF <- AllTweetsDF[,!(names(AllTweetsDF) %in% drops)]

table(AllTweetsDF$screen_name)

g1 <- ggplot(AllTweetsDF, aes(AllTweetsDF$screen_name, fill = AllTweetsDF$screen_name))+
  geom_bar(stat = "count")+
  theme(axis.text.x = element_text(angle = 45, vjust = 0.75))+
  theme(legend.position = "none")
g1
```
Feature engineering
```{r}
# TO DO: add columns: Name, Party
table(AllTweetsDF$screen_name)
AllTweetsDF$name <- AllTweetsDF$screen_name

AllTweetsDF$name <- revalue(AllTweetsDF$name, c("Haavisto" = "PekkaHaavisto", "Halla_aho" = "JussiHalla-Aho", "JaanaPelkonen" = "JaanaPelkonen", "MaiKivela" = "MaiKivela", "outialanko" = "OutiAlanko", "paavoarhinmaki" = "PaavoArhinmaki", "SebastianTyne" = "SebastianTynkkynen", "terhikoulumies" = "TerhiKoulumies"))

AllTweetsDF$party <- NA

for(i in 1:nrow(AllTweetsDF)){
  if(AllTweetsDF$name[i] %in% GreenParty) {
    AllTweetsDF$party[i] <- "GreenParty"
  }
}

for(i in 1:nrow(AllTweetsDF)){
  if(AllTweetsDF$name[i] %in% LeftAlliance) {
    AllTweetsDF$party[i] <- "LeftAlliance"
  }
}

for(i in 1:nrow(AllTweetsDF)){
  if(AllTweetsDF$name[i] %in% FinnsParty) {
    AllTweetsDF$party[i] <- "FinnsParty"
  }
}

for(i in 1:nrow(AllTweetsDF)){
  if(AllTweetsDF$name[i] %in% NCP) {
    AllTweetsDF$party[i] <- "NCP"
  }
}


```
Splitting the dataset and removing target variables
```{r}
splitindex <- createDataPartition(AllTweetsDF$name, p = 0.75, list = FALSE)
train <- AllTweetsDF[splitindex,]
test <- AllTweetsDF[-splitindex,]
```
Checking the cross validation between datasets on target values
```{r}
splitgg1 <- ggplot(train, aes(train$name, fill = train$name))+
  geom_bar(stat = "count")
splitgg2 <- ggplot(test, aes(test$name, fill = test$name))+
  geom_bar(stat = "count")
multiplot(splitgg1,splitgg2, cols = 1)
```
Train and test datasets are equally split.

On first algorithm we will predict from which political party the tweet has come. Later we will move onto predicting who tweeted that. As we want to only use the text as a predictor, we will remove party and name columns from test set (leaving only the text and empty party column) and name columns from train
```{r}
test$screen_name <- NULL
test$name <- NULL
test$party <- NA
train$screen_name <- NULL
train$name <- NULL

all <- rbind(train, test) #building combined all dataset for future use
```
### Preprocessing

Creating the DTM (Document Term Matrix) or also known as TFM (Term Frequency Matrix)
```{r}
prepFun <- tolower
tokFun <- tokenize_tweets # Using tokenizer specializied for tweets from Tokenizers package

tokenTrain <- train$text %>% # Pre processes for the tokenizing
  prepFun %>% 
  tokFun

itTrain <- itoken(tokenTrain) # Creating the tokens
vocab <- create_vocabulary(itTrain) #B Building a vocabulary from tokenized data

vectorizer = vocab_vectorizer(vocab)

dtmTrain = create_dtm(itTrain, vectorizer)
```
Fitting the first model
```{r}
fit <- glmnet(x = dtmTrain, y = train$party,
              family = "multinomial",
              type.multinomial = "grouped")


glmProbs <- predict(fit, newx = test$party, type = "class")
plot(fit)
```

