---
title: "TweetCollection"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, include=FALSE}
library(tidyverse)
library(rtweet)
library(purrr)
library(plyr)
library(text2vec)
library(caret)
library(randomForest)
library(caTools)
library(diveRsity)
library(tm)
library(tokenizers)
library(glmnet)
```

POC
```{r}
trump <- get_timeline("realDonaldTrump", n = 180)
```
the tweet collection gives us 90 different variables. Surely we dont need all of that so we are gonna cherry pick the most interesting ones
```{r}
toKeep <- c("screen_name", "text", "is_retweet")
trump <- trump[names(trump) %in% toKeep]
```
Next  I will choose the tweeters for this project. I'm aiming to choose people who are currently in parlament and equally people from each party.

* Outi Alanko-Kahiluoto @outialanko (Green party)
* Pekka Haavisto @Haavisto (Green Party)
* Paavo Arhinmäki @paavoarhinmaki (Left Alliance)
* Mai Kivelä @MaiKivela (Left Alliance)
* Jussi Halla-Aho @Halla_aho (Finns Party)
* Sebastian Tynkkynen @SebastianTyne (Finns Party)
* Terhi Koulumies @terhikoulumies (National Coalition Party)
* Jaana Pelkonen @JaanaPelkonen (National Coalition Party)

```{r}
Tweeters <- c("outialanko", "Haavisto", "paavoarhinmaki", "MaiKivela", "Halla_aho", "Sebastiantyne", "terhikoulumies", "JaanaPelkonen")
TweetersDF <- tibble(name = Tweeters)

GreenParty <- c("OutiAlanko", "PekkaHaavisto")
LeftAlliance <- c("PaavoArhinmaki", "MaiKivela")
FinnsParty <- c("JussiHalla-Aho", "SebastianTynkkynen")
NCP <- c("TerhiKoulumies", "JaanaPelkonen")
```

```{r}
tweetsAll <- TweetersDF %>% mutate(data = map(name, ~get_timeline(.x, n = 1000)))
list2env(tweetsAll, .GlobalEnv) # Separating the lists
AllTweetsDF <- bind_rows(data)  # Unlisting and binding the nested dataframes
```
Cleaning the data
```{r}
toKeep <- c("screen_name", "text", "is_retweet")
AllTweetsDF <- AllTweetsDF[,names(AllTweetsDF) %in% toKeep] # Choose only desired variables

AllTweetsDF <- AllTweetsDF[(AllTweetsDF$is_retweet == "FALSE"),]

drops <- c("is_retweet")
AllTweetsDF <- AllTweetsDF[,!(names(AllTweetsDF) %in% drops)] # Removing retweets

table(AllTweetsDF$screen_name)

g1 <- ggplot(AllTweetsDF, aes(AllTweetsDF$screen_name, fill = AllTweetsDF$screen_name))+
  geom_bar(stat = "count")+
  theme(axis.text.x = element_text(angle = 45, vjust = 0.75))+
  theme(legend.position = "none")
g1
```
Feature engineering
```{r}
# TO DO: add columns: Name, Party
table(AllTweetsDF$screen_name)
AllTweetsDF$name <- AllTweetsDF$screen_name

AllTweetsDF$name <- revalue(AllTweetsDF$name, c("Haavisto" = "PekkaHaavisto", "Halla_aho" = "JussiHalla-Aho", "JaanaPelkonen" = "JaanaPelkonen", "MaiKivela" = "MaiKivela", "outialanko" = "OutiAlanko", "paavoarhinmaki" = "PaavoArhinmaki", "SebastianTyne" = "SebastianTynkkynen", "terhikoulumies" = "TerhiKoulumies"))

AllTweetsDF$party <- NA

for(i in 1:nrow(AllTweetsDF)){
  if(AllTweetsDF$name[i] %in% GreenParty) {
    AllTweetsDF$party[i] <- "GreenParty"
  }
}

for(i in 1:nrow(AllTweetsDF)){
  if(AllTweetsDF$name[i] %in% LeftAlliance) {
    AllTweetsDF$party[i] <- "LeftAlliance"
  }
}

for(i in 1:nrow(AllTweetsDF)){
  if(AllTweetsDF$name[i] %in% FinnsParty) {
    AllTweetsDF$party[i] <- "FinnsParty"
  }
}

for(i in 1:nrow(AllTweetsDF)){
  if(AllTweetsDF$name[i] %in% NCP) {
    AllTweetsDF$party[i] <- "NCP"
  }
}


```
Splitting the dataset and removing target variables
```{r}
splitindex <- createDataPartition(AllTweetsDF$name, p = 0.75, list = FALSE)
train <- AllTweetsDF[splitindex,]
test <- AllTweetsDF[-splitindex,]
```
Checking the cross validation between datasets on target values
```{r}
splitgg1 <- ggplot(train, aes(train$name, fill = train$name))+
  geom_bar(stat = "count")
splitgg2 <- ggplot(test, aes(test$name, fill = test$name))+
  geom_bar(stat = "count")
multiplot(splitgg1,splitgg2, cols = 1)
```
Train and test datasets are equally split.

On first algorithm we will predict from which political party the tweet has come. Later we will move onto predicting who tweeted that. As we want to only use the text as a predictor, we will remove party and name columns from test set (leaving only the text and empty party column) and name columns from train
```{r}
origtestparty <- test$party # Storing the original party values to check accuracy later on
test$screen_name <- NULL
test$name <- NULL
test$party <- NA
train$screen_name <- NULL
train$name <- NULL

all <- rbind(train, test) #building combined all dataset for future use
```
### Preprocessing

Creating the DTM (Document Term Matrix) or also known as TFM (Term Frequency Matrix)
```{r}
prepFun <- tolower
tokFun <- tokenize_tweets # Using tokenizer specializied for tweets from Tokenizers package

tokenTrain <- train$text %>% # Pre processes for the tokenizing
  prepFun %>% 
  tokFun

itTrain <- itoken(tokenTrain) # Creating the tokens

# Tokenize the test data
tokenTest <- test$text %>% 
  prepFun %>% 
  tokFun

itTest <- itoken(tokenTest)

vocab <- create_vocabulary(itTrain) #B Building a vocabulary from tokenized data

vectorizer = vocab_vectorizer(vocab)

dtmTrain = create_dtm(itTrain, vectorizer)
```

#### LEAVING THE MODEL FOR NOW, IT'S NOT WORKING. MOVING INTO TF-IDF AND RETURNING HERE LATER ####

Fitting the first model
```{r}
#set.seed(205)
#fit <- glmnet(x = dtmTrain, y = train$party,
#              family = "multinomial",
#              type.multinomial = "grouped")
#
#plot(fit, xvar = "lambda", label = TRUE, type.coef = "2norm")
#
#cvfit = cv.glmnet(x = dtmTrain, y = train$party, family = "multinomial", type.multinomial = "grouped", parallel = TRUE)
#plot(cvfit)



#glmPred <- predict(cvfit, newx = test$party,  s = "lambda.min", type = "class")
```

TF-IDF
```{r}
tfidf <- TfIdf$new()
dtm_train_tfidf <- fit_transform(dtmTrain, tfidf)
dtm_test_tfidf = create_dtm(itTest, vectorizer) %>%
  transform(tfidf)
```
Model with new data
```{r}
set.seed(205)
glmnet_classifier <- cv.glmnet(x = dtm_train_tfidf, y = train$party,
                               family = "multinomial",
                               alpha = 0,
                               type.measure = "mae",
                               nfolds = 4,
                               thresh = 1e-3,
                               maxit = 1e3)

plot(glmnet_classifier)

preds <- predict(glmnet_classifier, dtm_test_tfidf, type = "response")

partyPred <- as.data.frame(preds)
head(partyPred)
```
Assigning the predictions into the test set
```{r}
max_names <- apply(partyPred, 1, which.max)
test$predicted <- max_names
parties <- c("FinnsParty", "GreenParty", "LeftAlliance", "NCP")
test$predicted <- mapvalues(test$predicted, from = c(1,2,3,4), to = parties)
```
Calculating the accuract by comparing to real party value
```{r}
test$party <- origtestparty

cat("The accuracy of the model predicting the party is", sum(test$party == test$predicted)/nrow(test))
```


